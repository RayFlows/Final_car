import torch
import cv2
import numpy as np
from config import cfg
from lib.models.pose_hrnet import get_pose_net
from core.inference import get_final_preds
from transforms import get_affine_transform

def load_model(cfg, model_path, device=torch.device('cpu')):
    """加载预训练模型"""
    model = get_pose_net(cfg, is_train=False)
    state = torch.load(model_path, map_location=device)
    model.load_state_dict(state, strict=False)
    model.to(device).eval()
    return model

def preprocess(frame, cfg):
    """预处理每一帧图像：Affine 变换 → 归一化 → 转 Tensor"""
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    h, w = img_rgb.shape[:2]

    # 计算 center 和 scale（与训练脚本一致）
    center = np.array([w / 2, h / 2], dtype=np.float32)
    aspect_ratio = cfg.MODEL.IMAGE_SIZE[0] / cfg.MODEL.IMAGE_SIZE[1]
    pixel_std = 200.0
    if w > aspect_ratio * h:
        h = w / aspect_ratio
    elif w < aspect_ratio * h:
        w = h * aspect_ratio
    scale = np.array([w / pixel_std, h / pixel_std], dtype=np.float32) * 1.25

    trans = get_affine_transform(center, scale, 0, cfg.MODEL.IMAGE_SIZE)
    inp = cv2.warpAffine(img_rgb, trans,
                         (cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1]),
                         flags=cv2.INTER_LINEAR)

    inp = inp.astype(np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
    inp = (inp - mean) / std

    inp = inp.transpose(2, 0, 1)[None, ...]  # HWC -> NCHW
    return torch.from_numpy(inp), center, scale, frame

def draw_results(frame, keypoints):
    """在图像上画出关键点"""
    img = frame.copy()
    for x, y in keypoints:
        cv2.circle(img, (int(x), int(y)), 4, (0, 255, 0), -1)
    
    cv2.imshow("Keypoints", img)

def process_video(video_path, model, cfg, device):
    """对视频流进行逐帧处理"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        inp_tensor, center, scale, orig_bgr = preprocess(frame, cfg)
        inp_tensor = inp_tensor.to(device)

        # 进行推理
        with torch.no_grad():
            output = model(inp_tensor)
        preds, _ = get_final_preds(
            cfg,
            output.cpu().numpy(),
            np.array([center], dtype=np.float32),
            np.array([scale], dtype=np.float32)
        )

        keypoints = preds[0]  # 获取 17 个关键点
        draw_results(orig_bgr, keypoints)

        # 按 'q' 键退出视频播放
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    # 加载配置
    args = SimpleNamespace(
        cfg="../experiments/coco/hrnet/w48_384x288_adam_lr1e-3_custom.yaml",
        opts=[],
        modelDir="", logDir="", dataDir="", prevModelDir=""
    )
    update_config(cfg, args)

    # 准备设备和模型
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = load_model(cfg, "../models/hrnet_fidip.pth", device)

    # 处理视频流
    video_path = 0  # 设置为 0 使用摄像头输入，或指定视频文件路径
    process_video(video_path, model, cfg, device)
